{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_tensorflow_jieun_ver1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/BLUENCE/M5_Idol_lyrics/blob/master/NLP/LSTM_tensorflow_jieun_ver1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "26FgpogPOeRn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 각자의 csv 파일을 공유 드라이브에 올린 후, 콜랩에서 불러올 수 있게 하기\n",
        "# 출처: http://studycolab.blogspot.com/2018/05/colaboratory-load-file-from-google-drive.html\n",
        "\n",
        "!pip install -U -q PyDrive\n",
        "\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "\n",
        "from pydrive.drive import GoogleDrive\n",
        "\n",
        "from google.colab import auth\n",
        "\n",
        "from oauth2client.client import GoogleCredentials \n",
        "\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# key 입력해서 본인 계정으로 로그인 해주세요"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s2SWi9xbCgml",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 공유폴더에 csv올려주세요\n",
        "# 올라가있는 csv파일경로로 가서 우클릭,\n",
        "# '공유 가능한 링크 가져오기'누르면 나오는 주소 중 id= 뒷부분이 file_id 입니다:\n",
        "# 예시) https://drive.google.com/open?id=1JeG6PWdug220IVHlCDe4v_ecgj9e0KEK\n",
        "\n",
        "file_id = '1JeG6PWdug220IVHlCDe4v_ecgj9e0KEK'\n",
        "\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "\n",
        "downloaded.GetContentFile('vc_total_speed_seoul.csv')  # 구글 드라이브에 새롭게 업로드\n",
        "# 앞으로 부를 파일명으로 맘대로 적어주심 됩니다.\n",
        "# 밑에서 해당 파일명으로 불러올거에요."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XDf9kQ8Nm4Cl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 지은 자료\n",
        "downloaded = drive.CreateFile({'id':'1NASD4unGUYhYl1MwxTNMio5j5Zs4Q9vg'})\n",
        "downloaded.GetContentFile('all_sentences_for_word2vec.txt')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':'1TwuGgAqOIFllzT-VzYgprlc92dDfqLBv'})\n",
        "downloaded.GetContentFile('all_word2vec_model')\n",
        "\n",
        "downloaded = drive.CreateFile({'id':'1e6rcHJvr5hXd4rue1PywUhMnE-7quSbr'})\n",
        "downloaded.GetContentFile('all_word2vec_model.wv.vectors.npy')\n",
        "\n",
        "\n",
        "downloaded = drive.CreateFile({'id':'1NWAy0mYmY0ttjECi755CgqVwIBQC2RAd'})\n",
        "downloaded.GetContentFile('all_word2vec_model.trainables.syn1neg.npy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uaLBgrKh5_Uh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "74YrEOQSpvIm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 참고사이트 1 : http://adventuresinmachinelearning.com/recurrent-neural-networks-lstm-tutorial-tensorflow/\n",
        "# 2 : https://aikorea.org/blog/rnn-tutorial-4/\n",
        "# 3 : https://github.com/BLUENCE/TensorFlow-Tutorials/blob/master/10%20-%20RNN/01%20-%20MNIST.py\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "import tensorflow.contrib.layers as layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9IhAatz_WLF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Phrases\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.word2vec import LineSentence\n",
        "\n",
        "class TextLoader:\n",
        "    def __init__(self, path):\n",
        "        with open(path, \"r\", encoding='utf-8') as _file:\n",
        "            self.text = _file.read().split()\n",
        "            # self.text = self.text[:10000]\n",
        "\n",
        "        self.song2vec = Word2Vec.load(\"all_word2vec_model\")\n",
        "        self.vocab, self.words = self.build_vocab()\n",
        "\n",
        "        self.X = self.text[:] # 텍스트 파일 전체를 복사 - self.text 와 같은 의미\n",
        "        self.y = [self.text[0]] + self.text[1:] # \n",
        "\n",
        "    def build_vocab(self):\n",
        "        vocab_inv = list(self.song2vec.wv.vocab.keys()) # key 값을 리스트화 / 글자만\n",
        "        vocab = {x: i for i, x in enumerate(vocab_inv)} # 0,1,2 등 인덱스와 단어를 dict 로 매칭시켜놓음\n",
        "        return vocab, vocab_inv\n",
        "\n",
        "    def next_batch(self, batch_size, seq_length):\n",
        "        start = np.random.randint(0, len(self.X)-batch_size*seq_length) # 랜덤으로 위치를 정함 - 끝의 값을 구하면 안됨 / 시작 위치를 글자를 다 배치사이즈와 시퀀스렝스로 구함 // 마지막까지는 안가겠다는 뜻\n",
        "        end   = start + batch_size*seq_length # 몇 단어를 가져올지\n",
        "\n",
        "        X_words = self.X[start:end]# 말그대로 글자\n",
        "        y_words = self.y[start:end]\n",
        "\n",
        "        X_idx = np.empty((batch_size, seq_length), dtype=np.int64) # 글자의 인덱스\n",
        "        y_idx = np.empty((batch_size, seq_length), dtype=np.int64)\n",
        "        X_wv = np.empty((batch_size, seq_length, 100)) # 글자의 word2vec \n",
        "        y_wv = np.empty((batch_size, seq_length, 100))\n",
        "        # 위에서 만들어준 자리에 따라 (저장공간 설정하는 과정) 아래에서 for 문을 돌며 값을 가져옴 / 그냥 하면 안되는 이유 : append는 느림, numpy의 경우에는 1 2 3 4 붙어있어야 함 / 5를 넣는다 하면 이걸 어딘가 복사해서 5를 붙여야 함.\n",
        "        for i in range(batch_size):\n",
        "            for j in range(seq_length):\n",
        "                X_idx[i, j] = self.vocab[X_words[i*seq_length+j]]\n",
        "                y_idx[i, j] = self.vocab[y_words[i*seq_length+j]]\n",
        "\n",
        "                X_wv[i, j] = self.song2vec.wv[X_words[i*seq_length+j]]\n",
        "                y_wv[i, j] = self.song2vec.wv[y_words[i*seq_length+j]]\n",
        "\n",
        "        return X_wv, X_idx, y_wv, y_idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d4RD6OF7_YNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow.contrib.rnn as rnn\n",
        "import tensorflow.contrib.layers as layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IxUegtp5_cz2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_layers  = 3\n",
        "hidden_size = 512\n",
        "batch_size  = 200\n",
        "max_length  = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "loader = TextLoader(\"all_sentences_for_word2vec.txt\")\n",
        "vocab_size = len(loader.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1AZI2_d8NUh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "2a67772b-4d07-448f-9cda-31cc4f219c04"
      },
      "cell_type": "code",
      "source": [
        "# with tf.device(\"/gpu:5\"):\n",
        "X = tf.placeholder(tf.float32, [None, max_length, 100])\n",
        "y = tf.placeholder(tf.int32, [None, max_length]) # 인풋은 워드투벡 아웃풋은 단어?\n",
        "    \n",
        "y_one_hot = tf.one_hot(y, vocab_size)\n",
        "\n",
        "# RNN 에 학습에 사용할 셀을 생성해준다. \n",
        "cells = [rnn.BasicLSTMCell(hidden_size) for _ in range(num_layers)]\n",
        "\n",
        "# 셀을 종합해서 RNN 을 정의합니다. \n",
        "cells = rnn.MultiRNNCell(cells, state_is_tuple=True)\n",
        "\n",
        "# 초기 state값을 0으로 초기화합니다.\n",
        "initial_state = cells.zero_state(batch_size, tf.float32)\n",
        "\n",
        "# outputs = [batch_size, max_length, hidden_size]\n",
        "outputs, states = tf.nn.dynamic_rnn(cells, X, initial_state=initial_state, dtype=tf.float32)\n",
        "\n",
        "# outputs 을 [batch_size*max_length, hidden_size] 형태로 바꿔줌\n",
        "outputs = tf.reshape(outputs, [-1, hidden_size])\n",
        "\n",
        "# 최종 출력값 설정하기\n",
        "logits = layers.linear(outputs, vocab_size)\n",
        "y_flat = tf.reshape(y_one_hot, [-1, vocab_size])\n",
        "\n",
        "\n",
        "# 손실함수를 정의해준다\n",
        "loss_op = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_flat)\n",
        "loss_op = tf.reduce_mean(loss_op)\n",
        "\n",
        "# 옵티마이저\n",
        "opt = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_op)\n",
        "\n",
        "y_softmax = tf.nn.softmax(logits)\n",
        "pred = tf.argmax(y_softmax, axis=1)\n",
        "pred = tf.reshape(pred, [batch_size, -1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-734462eeff30>:30: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ngJLMuneG7kB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 세션을 만들고 학습 진행\n",
        "\n",
        "# saver = tf.train.Saver()\n",
        "\n",
        "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "# model_filepath=\n",
        "\n",
        "with tf.Session(config=sess_config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "#     saver.restore(sess, \"char-rnn_230000\") # \n",
        "\n",
        "#   for step in range(230000, 1000000):\n",
        "    for step in range(0, 230):\n",
        "        X_wv, X_idx, y_wv, y_idx = loader.next_batch(batch_size, max_length)\n",
        "        loss, _ = sess.run([loss_op, opt], feed_dict={X: X_wv, y: y_idx})\n",
        "\n",
        "        if (step+1) % 10 == 0:\n",
        "            print(\"{:08d} step, loss:{:.4f}\".format(step+1, loss))\n",
        "\n",
        "            random = np.random.randint(0, batch_size)\n",
        "            results = sess.run(pred, feed_dict={X: X_wv})\n",
        "            words = [loader.words[word] for word in results[random]]\n",
        "            print(\" \".join(words))\n",
        "\n",
        "#         if (step+1) % 1000 == 0: \n",
        "#             saver.save(sess, \"char-rnn_\"+str(step+1)) # 원래 checkpoints/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zmffPK_GKSUd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TEST EXAMPLE"
      ]
    },
    {
      "metadata": {
        "id": "nEkWC9ryKZgL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_layers = 3\n",
        "hidden_size = 512\n",
        "batch_size  = 200 # 1글자\n",
        "max_length  = 1 \n",
        "\n",
        "loader = TextLoader(\"all_sentences_for_word2vec.txt\")\n",
        "vocab_size = len(loader.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zglSFixCKh08",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_softmax = tf.nn.softmax(logits)\n",
        "pred = tf.argmax(y_softmax, axis=1)\n",
        "pred = tf.reshape(pred, [batch_size, -1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nBJCQgRXK1ky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2172
        },
        "outputId": "f5adea4c-6312-4f03-b896-fcd5860f0716"
      },
      "cell_type": "code",
      "source": [
        "# 시작 글자 생성\n",
        "sentence = [\"안녕\", \"나는\", \"여자\"]\n",
        "print(sentence)\n",
        "print(\"Start with:\", \" \".join(sentence))\n",
        "\n",
        "# saver = tf.train.Saver()\n",
        "sess_config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
        "\n",
        "with tf.Session(config=sess_config) as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "#     saver.restore(sess, \"char-rnn\")\n",
        "    \n",
        "# [배치사이즈, max_length, 100]\n",
        "    vec = np.empty((1, len(sentence), 100)) # 시작 글자를 주고 다음 단어를 예측 - 시작으로 준 것을 전부 다 입력으로 넣겠다\n",
        "    for i, word in enumerate(sentence):\n",
        "        vec[:, i, :] = loader.song2vec.wv[word]\n",
        "    \n",
        "# 매 이터레이션마다 글자 하나씩 생성\n",
        "    state = sess.run(states, feed_dict={X: vec}) #입력단어 sentence 이후에 들어올 단어를 예측\n",
        "    for i in range(15): # for문을 돌면서 풀어헤치는 중!!\n",
        "        vec = loader.song2vec.wv[sentence[-1]].reshape(1, 1, 100)\n",
        "        \n",
        "        pred_char, state = sess.run([pred, states], \n",
        "            feed_dict={X: vec, initial_state: state}) # 원래 initial stete 는 0 이었으나\n",
        "# 이전 스텝에 갖고 있는 state 값을 다음 스텝에 넣어줌 // 입력단어들을 그 다음 스텝에 넣어주는 것임\n",
        "        \n",
        "        pred_char = loader.words[pred_char[0][-1]]\n",
        "        sentence.append(pred_char)\n",
        "\n",
        "for i, word in enumerate(sentence):\n",
        "    print(word, end=\" \")\n",
        "    if (i+1) % 5 == 0:\n",
        "        print()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['안녕', '나는', '여자']\n",
            "Start with: 안녕 나는 여자\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,3,100]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,3,100], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-7a1e8c61748e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# 매 이터레이션마다 글자 하나씩 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#입력단어 sentence 이후에 들어올 단어를 예측\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# for문을 돌면서 풀어헤치는 중!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msong2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1291\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,3,100]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,3,100], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Placeholder', defined at:\n  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-11-82d743c071f2>\", line 4, in <module>\n    X = tf.placeholder(tf.float32, [None, max_length, 100])\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py\", line 1735, in placeholder\n    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 4925, in placeholder\n    \"Placeholder\", dtype=dtype, shape=shape, name=name)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'Placeholder' with dtype float and shape [?,3,100]\n\t [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=[?,3,100], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "mvUm1amW_nQd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A60llx5T_nSt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hwsXybR7_m1L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}